In our project, we decided to take a parsimonious approach where we delve into the simpler things first and only increased the level of complexity when necessary, so as to follow Occam's razor. The initial part of the project involved setting up the structure of the data for training, validation, and testing. We split our data into a 70-30 ratio (training and validation to internal test before the blind test). The 70-30 training-test split was preferred over the 80-20 due to the few observations in the dataset. The training section is not a problem since we can extract as much as possible with validation techniques. However, we needed to have as many observations for the internal test as possible since this is the most important approximation to the blind test.
The final selected models in our project contain two neural network models implemented with different parameters in the search. The 'complex' search contained a higher number of hidden layers and units, while the 'simpler' approach contained fewer of each. The optimizer option is also limited to Adam and SGD in the 'simpler' approach, and the hyperparameters for the learning algorithm were also limited.
In terms of the comparison models, we chose a K-nearest regression model,  a Kernel Ridge regression model and a Random Forest model all implemented with scikit-learn. We also implemented variations of Linear regressions with and without regularization methods like Ridge and Lasso from scikit-learn.
For model selection, we used K-fold cross-validation as a standard validation technique with k = 10. This setting allowed us to have a good trade-off between bias and variance in estimations. We also explored the Leave-One-Out cross-validation technique but ultimately dismissed it since the variance in estimation was too high to compare different results between models.
As mentioned, the model assessment was done with the 30% of data. This split was dedicated only to assess the best models at the end of the project when the models were already finished. It is important to note that after we did the model assessment, we retrained the selected best models with the whole dataset to make predictions on the blind test set.