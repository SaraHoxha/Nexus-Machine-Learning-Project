# Abstract for nexus team
In our project we decided to take a parsimonious approach where we delve into the simpler things first and only increased the level of complexity if it was necessary, this was in order to follow occam's razor, the initial part of the project was setting up the structure of the data in order to do training, validation and test, we splitted our data in a 70(Training and validation), 30(Internal test before the blind test), the 70-30 training-test split was preferred above the 80-20 because of the few observations in the data set, the training section is not a problem since we can extract as much as possible with the validation techniques, but we needed to have as much observation for the internal test as possible since this is the most important approximation we have to the blind test.

The final selected models in our project contain, two neural network models implemented with different parameters in the search, the 'complex' search contained a higher number of hidden layers and higher number of units, while the 'simpler' approach contained few of each, the optimizer option is also limited to adam and SGD in the 'simpler' approach, and the hyperparameters for the learning algorithm were also limited.

In terms of the comparison models, we chose, a K-nearest regression model implemented with scikit-learn, a Kernel ridge regression model implemented with scikit-learn, a Random forest model implemented with scikit-learn, we also implemented variations of Linear regressions with and without regularization methods like Ridge and Lasso from scikit-learn.

For model selection we used K-fold cross validation as a standard validation technique, with k = 10, this setting allowed us to have a good trade off between bias and variance in estimations, we also explored the Leave-one-out cross validation technique but ultimately discarded it since the variance in estimation was too high to compare different results between models.

The model assesment was done with the 30% of the data already explained, this split was dedicated only to assess the best models at the end of the project when the models where already finished, no other changes to make, is important to note that after we did the model assesment, we retrain the selected best models with the whole data set to do the predictions on the blind test set.